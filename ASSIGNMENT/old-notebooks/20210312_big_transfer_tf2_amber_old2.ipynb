{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "20210312_big_transfer_tf2_amber",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/barksdaleaz/big_transfer/blob/master/20210312_big_transfer_tf2_amber.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClOqe_6GHB_3"
      },
      "source": [
        "##### Copyright 2020 Google LLC.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHsFiv-UHFBq",
        "cellView": "form"
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/google-research/big_transfer/blob/master/colabs/big_transfer_tf2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXY0spPLdxkX"
      },
      "source": [
        "# BigTransfer (BiT): A step-by-step tutorial for state-of-the-art vision\n",
        "\n",
        "The original notebook for this tutorial in Big Transfer transfer learning was created by Jessica Yung and Joan Puigcerver of Google Brain in Zürich.  As part of New York University's CS-GY 6613 Introduction to AI with Professor Pantelis Monogioudis, it has been altered by Amber Barksdale and Russell Wustenberg.\n",
        "\n",
        "In this lab, we will load the BiT-M model trained on the ImageNet-21k dataset with a ResNet50x3 architecutre.  It is based on the [BigTransfer paper](https://arxiv.org/abs/1912.11370) and accompanied by our own reflections on transfer learning, co-located in this repository.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pitZxiw5hHYq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "outputId": "9eac233e-957b-4d20-a5db-cf3023f98b20"
      },
      "source": [
        "#@title Imports\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "import time\n",
        "\n",
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "import pathlib\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "# Load the TensorBoard notebook extension.\n",
        "%load_ext tensorboard\n",
        "from datetime import datetime\n",
        "import io\n",
        "import itertools\n",
        "from packaging import version\n",
        "\n",
        "from tensorflow import keras\n",
        "\n",
        "import sklearn.metrics\n",
        "\n",
        "print(\"TensorFlow version: \", tf.__version__)\n",
        "assert version.parse(tf.__version__).release[0] >= 2, \\\n",
        "    \"This notebook requires TensorFlow 2.0 or above.\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow version:  2.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7i3p4Zmr0J0X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c2359f4-28c5-486c-a892-a85e4f75e7b3"
      },
      "source": [
        "#@title Construct imagenet logit-to-class-name dictionary (imagenet_int_to_str)\n",
        "\n",
        "!wget https://storage.googleapis.com/bit_models/imagenet21k_wordnet_lemmas.txt\n",
        "\n",
        "imagenet_int_to_str = {}\n",
        "\n",
        "with open('imagenet21k_wordnet_lemmas.txt', 'r') as f:\n",
        "  for i in range(1000):\n",
        "    row = f.readline()\n",
        "    row = row.rstrip()\n",
        "    imagenet_int_to_str.update({i: row})"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-03-12 19:11:14--  https://storage.googleapis.com/bit_models/imagenet21k_wordnet_lemmas.txt\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.135.128, 74.125.142.128, 74.125.195.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.135.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 522999 (511K) [text/plain]\n",
            "Saving to: ‘imagenet21k_wordnet_lemmas.txt.4’\n",
            "\n",
            "\r          imagenet2   0%[                    ]       0  --.-KB/s               \rimagenet21k_wordnet 100%[===================>] 510.74K  --.-KB/s    in 0.003s  \n",
            "\n",
            "2021-03-12 19:11:14 (160 MB/s) - ‘imagenet21k_wordnet_lemmas.txt.4’ saved [522999/522999]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3DBgilDnevv"
      },
      "source": [
        "#@title CIFAR-10 Label Names\n",
        "cifar_10_labels = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W97ueEAtk9k4"
      },
      "source": [
        "# Load the pre-trained BiT model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jw8LOz3NgNG3"
      },
      "source": [
        "From the 10 available models, we chose to use a ResNet50x3 trained on ImageNet-21k.  We chose this model because it provides a complex enough architecture to see valid test results after fine-stream tuning, but simple enough to run on a publicly available Google CoLab notebook.  Available models "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1i8HCwx28Ya2"
      },
      "source": [
        "# Load model into KerasLayer\n",
        "model_url = \"https://tfhub.dev/google/bit/m-r50x3/1\"\n",
        "module = hub.KerasLayer(model_url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewOVUVsvnSpo",
        "cellView": "form"
      },
      "source": [
        "#@title Helper functions for loading image (hidden)\n",
        "\n",
        "def preprocess_image(image):\n",
        "  image = np.array(image)\n",
        "  # reshape into shape [batch_size, height, width, num_channels]\n",
        "  img_reshaped = tf.reshape(image, [1, image.shape[0], image.shape[1], image.shape[2]])\n",
        "  # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n",
        "  image = tf.image.convert_image_dtype(img_reshaped, tf.float32)  \n",
        "  return image\n",
        "\n",
        "def load_image_from_url(url):\n",
        "  \"\"\"Returns an image with shape [1, height, width, num_channels].\"\"\"\n",
        "  response = requests.get(url)\n",
        "  image = Image.open(BytesIO(response.content))\n",
        "  image = preprocess_image(image)\n",
        "  return image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8fDpUyl75pT",
        "cellView": "form"
      },
      "source": [
        "#@title Plotting helper functions (hidden)\n",
        "#@markdown Credits to Xiaohua Zhai, Lucas Beyer and Alex Kolesnikov from Brain Zurich, Google Research\n",
        "\n",
        "# Show the MAX_PREDS highest scoring labels:\n",
        "MAX_PREDS = 5\n",
        "# Do not show labels with lower score than this:\n",
        "MIN_SCORE = 0.8 \n",
        "\n",
        "def show_preds(logits, image, correct_cifar_label=None, cifar_logits=False):\n",
        "\n",
        "  if len(logits.shape) > 1:\n",
        "    logits = tf.reshape(logits, [-1])\n",
        "\n",
        "  fig, axes = plt.subplots(1, 2, figsize=(7, 4), squeeze=False)\n",
        "\n",
        "  ax1, ax2 = axes[0]\n",
        "\n",
        "  ax1.axis('off')\n",
        "  ax1.imshow(image)\n",
        "  if correct_cifar_label is not None:\n",
        "    ax1.set_title(cifar_10_labels[correct_cifar_label])\n",
        "  classes = []\n",
        "  scores = []\n",
        "  logits_max = np.max(logits)\n",
        "  softmax_denominator = np.sum(np.exp(logits - logits_max))\n",
        "  for index, j in enumerate(np.argsort(logits)[-MAX_PREDS::][::-1]):\n",
        "    score = 1.0/(1.0 + np.exp(-logits[j]))\n",
        "    if score < MIN_SCORE: break\n",
        "    if not cifar_logits:\n",
        "      # predicting in imagenet label space\n",
        "      classes.append(imagenet_int_to_str[j])\n",
        "    else:\n",
        "      # predicting in tf_flowers label space\n",
        "      classes.append(cifar_10_labels[j])\n",
        "    scores.append(np.exp(logits[j] - logits_max)/softmax_denominator*100)\n",
        "\n",
        "  ax2.barh(np.arange(len(scores)) + 0.1, scores)\n",
        "  ax2.set_xlim(0, 100)\n",
        "  ax2.set_yticks(np.arange(len(scores)))\n",
        "  ax2.yaxis.set_ticks_position('right')\n",
        "  ax2.set_yticklabels(classes, rotation=0, fontsize=14)\n",
        "  ax2.invert_xaxis()\n",
        "  ax2.invert_yaxis()\n",
        "  ax2.set_xlabel('Prediction probabilities', fontsize=11)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xRFWdwHrNt9"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRDwXBLzFYFU"
      },
      "source": [
        "Our BiT-M currently is trained for the ImageNet-21k classification labels.  It does not have the proper logit-to-label output for our target dataset the [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html), which has also been used in [other tutorials](https://www.tensorflow.org/tutorials/load_data/images). This dataset contains 60,000 images of 10 classes.\n",
        "\n",
        "In order to re-map the logits to the correct labels, we must *fine tune* the model onto the new output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_KTcxDYhZf_",
        "cellView": "both"
      },
      "source": [
        "# Import CIFAR-10 data from tfds\n",
        "\n",
        "# using below to load entire data set for test purposes only\n",
        "ds, info2 = tfds.load(\n",
        "    name='cifar10',\n",
        "    split='train',\n",
        "    with_info=True,\n",
        ")\n",
        "\n",
        "# using below to load only 100 images\n",
        "train_10_20_ds, info = tfds.load(\n",
        "    name='cifar10', \n",
        "    split='train[10:110]',\n",
        "    with_info=True\n",
        "    )\n",
        "\n",
        "num_examples = info.splits['train[10:110]'].num_examples\n",
        "num_examples2 = info2.splits['train'].num_examples\n",
        "NUM_CLASSES = 10\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIJyrvKX223E"
      },
      "source": [
        "# Split into train and test sets\n",
        "train_split = 0.9\n",
        "num_train = int(train_split * num_examples2)\n",
        "\n",
        "# Train_10_20_DS_Train should equal 100 images\n",
        "train_10_20_ds_train = train_10_20_ds.take(100)\n",
        "\n",
        "# here we use DS_TEST because it holds the entire 50,000 set\n",
        "ds_test = ds.skip(num_train)\n",
        "\n",
        "# DATASET_NUM_TRAIN_EXAMPLES = num_examples\n",
        "DATASET_NUM_TRAIN_EXAMPLES = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXZXPPVShbb9"
      },
      "source": [
        "## Fine-tuning the BiT model\n",
        "\n",
        "As in the BiT tutorial, we are going to fine-tune the BiT model so it performs better on the CIFAR-10 dataset (`cifar_10`). \n",
        "\n",
        "There are two steps:\n",
        "1. Create a new model with a new final layer (which is called the ‘head’), and\n",
        "2. Fine-tune this model using BiT-HyperRule, the hyperparameter heuristic proposed by BiT.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2RGgy4ghgsh"
      },
      "source": [
        "#@title BiT With a new head\n",
        "# Add new head to the BiT model\n",
        "\n",
        "class MyBiTModel(tf.keras.Model):\n",
        "  \"\"\"BiT with a new head.\"\"\"\n",
        "\n",
        "  def __init__(self, num_classes, module):\n",
        "    super().__init__()\n",
        "\n",
        "    self.num_classes = num_classes\n",
        "    self.head = tf.keras.layers.Dense(num_classes, kernel_initializer='zeros')\n",
        "    self.bit_model = module\n",
        "  \n",
        "  def call(self, images):\n",
        "    # No need to cut head off since we are using feature extractor model\n",
        "    bit_embedding = self.bit_model(images)\n",
        "    return self.head(bit_embedding)\n",
        "\n",
        "model = MyBiTModel(num_classes=NUM_CLASSES, module=module)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEYhH53WdmjJ"
      },
      "source": [
        "### Data and preprocessing\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jp5APPRlheCC"
      },
      "source": [
        "#@title Appling the BiT HyperRule\n",
        "\n",
        "#@markdown The BiT-HyperRule is a heuristic for setting hyperparameters for the\n",
        "#@markdown BiT models.  It takes a number of decisions into account for optimization\n",
        "#@markdown for a full overview of the code, [see here](https://github.com/google-research/big_transfer).\n",
        "#@markdown Due to the low-shot nature of our training in this example, we will not\n",
        "#@markdown be utilizing mixup, which is advised by BiT for only tasks > 20,000 samples. \n",
        "#@markdown \n",
        "#@markdown The creators of the tutorial utilized a pre-programmed dropdown menu for\n",
        "#@markdown HyperRule selection and we have chosen to leave this intact for our experiment.\n",
        "\n",
        "IMAGE_SIZE = \"=\\u003C96x96 px\" #@param [\"=<96x96 px\",\"> 96 x 96 px\"]\n",
        "DATASET_SIZE = \"\\u003C20k examples\" #@param [\"<20k examples\", \"20k-500k examples\", \">500k examples\"]\n",
        "\n",
        "if IMAGE_SIZE == \"=<96x96 px\":\n",
        "  RESIZE_TO = 160\n",
        "  CROP_TO = 128\n",
        "else:\n",
        "  RESIZE_TO = 512\n",
        "  CROP_TO = 480\n",
        "\n",
        "if DATASET_SIZE == \"<20k examples\":\n",
        "  SCHEDULE_LENGTH = 500\n",
        "  SCHEDULE_BOUNDARIES = [200, 300, 400]\n",
        "elif DATASET_SIZE == \"20k-500k examples\":\n",
        "  SCHEDULE_LENGTH = 10000\n",
        "  SCHEDULE_BOUNDARIES = [3000, 6000, 9000]\n",
        "else:\n",
        "  SCHEDULE_LENGTH = 20000\n",
        "  SCHEDULE_BOUNDARIES = [6000, 12000, 18000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DiIrQFBhe9R",
        "cellView": "form"
      },
      "source": [
        "#@title Preprocessing Helper Functions\n",
        "# Preprocessing helper functions\n",
        "\n",
        "# Create data pipelines for training and testing:\n",
        "# Batch size was decreased due to an OOM error (Resource Exhausted)\n",
        "BATCH_SIZE = 200\n",
        "SCHEDULE_LENGTH = SCHEDULE_LENGTH * 512 / BATCH_SIZE\n",
        "\n",
        "STEPS_PER_EPOCH = 10\n",
        "\n",
        "def cast_to_tuple(features):\n",
        "  return (features['image'], features['label'])\n",
        "  \n",
        "def preprocess_train(features):\n",
        "  # Apply random crops and horizontal flips for all tasks \n",
        "  # except those for which cropping or flipping destroys the label semantics\n",
        "  # (e.g. predict orientation of an object)\n",
        "  features['image'] = tf.image.random_flip_left_right(features['image'])\n",
        "  features['image'] = tf.image.resize(features['image'], [RESIZE_TO, RESIZE_TO])\n",
        "  features['image'] = tf.image.random_crop(features['image'], [CROP_TO, CROP_TO, 3])\n",
        "  features['image'] = tf.cast(features['image'], tf.float32) / 255.0\n",
        "  return features\n",
        "\n",
        "def preprocess_test(features):\n",
        "  features['image'] = tf.image.resize(features['image'], [RESIZE_TO, RESIZE_TO])\n",
        "  features['image'] = tf.cast(features['image'], tf.float32) / 255.0\n",
        "  return features\n",
        "\n",
        "pipeline_train = (train_10_20_ds_train\n",
        "                  .shuffle(10000)\n",
        "                  .repeat(int(SCHEDULE_LENGTH * BATCH_SIZE / DATASET_NUM_TRAIN_EXAMPLES * STEPS_PER_EPOCH) + 1 + 50)  # repeat dataset_size / num_steps\n",
        "                  .map(preprocess_train, num_parallel_calls=8)\n",
        "                  .batch(BATCH_SIZE)\n",
        "                  .map(cast_to_tuple)  # for keras model.fit\n",
        "                  .prefetch(2))\n",
        "\n",
        "# note that ds_test is used for the testing pipeline, whereas\n",
        "# train_10_20_ds_train is used only for the training\n",
        "pipeline_test = (ds_test.map(preprocess_test, num_parallel_calls=1)\n",
        "                  .map(cast_to_tuple)  # for keras model.fit\n",
        "                  .batch(BATCH_SIZE)\n",
        "                  .prefetch(2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xU1SoL32eD4f"
      },
      "source": [
        "### Fine-tuning loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmckWR7wz2d2"
      },
      "source": [
        "The below code will fine tune the model using our reduced training sample set taken from the CIFAR-10.  There are between 1 and 10 samples from each class contained within, and expected accuracy given such conditions will be between 75% and 88% accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zx60lxfzhoTQ"
      },
      "source": [
        "# Define optimiser and loss\n",
        "\n",
        "lr = 0.003 * BATCH_SIZE / 512 \n",
        "\n",
        "# Decay learning rate by a factor of 10 at SCHEDULE_BOUNDARIES.\n",
        "lr_schedule = tf.keras.optimizers.schedules.PiecewiseConstantDecay(boundaries=SCHEDULE_BOUNDARIES, \n",
        "                                                                   values=[lr, lr*0.1, lr*0.001, lr*0.0001])\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=lr_schedule, momentum=0.9)\n",
        "\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJmqsAFmeQZd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89b1257b-bf02-4f3e-ef48-565cc537f41e"
      },
      "source": [
        "model.compile(optimizer=optimizer,\n",
        "              loss=loss_fn,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Fine-tune model\n",
        "history = model.fit(\n",
        "    pipeline_train,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    steps_per_epoch=STEPS_PER_EPOCH,\n",
        "    epochs=10,\n",
        "    #epochs= int(SCHEDULE_LENGTH / STEPS_PER_EPOCH),  # TODO: replace with `epochs=10` here to shorten fine-tuning for tutorial if you wish\n",
        "    validation_data=pipeline_test  # here we are only using \n",
        "                                   # this data to evaluate our performance\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "10/10 [==============================] - 55s 6s/step - loss: 0.0191 - accuracy: 0.9998 - val_loss: 0.3475 - val_accuracy: 0.8846\n",
            "Epoch 2/10\n",
            "10/10 [==============================] - 49s 5s/step - loss: 0.0043 - accuracy: 0.9992 - val_loss: 0.3406 - val_accuracy: 0.8920\n",
            "Epoch 3/10\n",
            "10/10 [==============================] - 49s 5s/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.3469 - val_accuracy: 0.8928\n",
            "Epoch 4/10\n",
            "10/10 [==============================] - 49s 5s/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.3478 - val_accuracy: 0.8928\n",
            "Epoch 5/10\n",
            "10/10 [==============================] - 49s 5s/step - loss: 7.4635e-04 - accuracy: 1.0000 - val_loss: 0.3491 - val_accuracy: 0.8908\n",
            "Epoch 6/10\n",
            "10/10 [==============================] - 49s 5s/step - loss: 6.6605e-04 - accuracy: 1.0000 - val_loss: 0.3504 - val_accuracy: 0.8908\n",
            "Epoch 7/10\n",
            "10/10 [==============================] - 49s 5s/step - loss: 7.5462e-04 - accuracy: 1.0000 - val_loss: 0.3501 - val_accuracy: 0.8906\n",
            "Epoch 8/10\n",
            "10/10 [==============================] - 49s 5s/step - loss: 6.9482e-04 - accuracy: 1.0000 - val_loss: 0.3487 - val_accuracy: 0.8916\n",
            "Epoch 9/10\n",
            "10/10 [==============================] - 49s 5s/step - loss: 7.0192e-04 - accuracy: 1.0000 - val_loss: 0.3459 - val_accuracy: 0.8924\n",
            "Epoch 10/10\n",
            "10/10 [==============================] - 49s 5s/step - loss: 7.9195e-04 - accuracy: 1.0000 - val_loss: 0.3441 - val_accuracy: 0.8930\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfsLX0um14ap"
      },
      "source": [
        "As seen above, the validation accuracy is within the expected limits (regularly scoring on multiple runs between .868 and .886).  The model took few turns (typically 4 or fewer) to reach 100% accuracy on the test set at which point the learning rate plateaued and accuracy and loss remained stable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0ahkv3ZI0Wa"
      },
      "source": [
        "## Save fine-tuned model for later use\n",
        "\n",
        "The following cell saves the fine tuned model for later use.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOYOLzWblDSf"
      },
      "source": [
        "# Save fine-tuned model as SavedModel\n",
        "export_module_dir = '/tmp/my_saved_bit_model/'\n",
        "tf.saved_model.save(model, export_module_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmIo8r_DzM6n"
      },
      "source": [
        "The following cell loads the fine tuned model for use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlgQVjWvlf_6"
      },
      "source": [
        "# Load saved model\n",
        "saved_module = hub.KerasLayer(export_module_dir, trainable=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5vu4H4dzQt-"
      },
      "source": [
        "As can be seen in the predictions below, images run through the model are now correlated to their proper CIFAR-10 classifications.  The model has been tuned and is ready for further experimentation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMKx3A0Z0h6E"
      },
      "source": [
        "# Visualise predictions from new model\n",
        "\n",
        "test_accuracy = tf.keras.metrics.Accuracy()\n",
        "\n",
        "# Show the MAX_PREDS highest scoring labels:\n",
        "MAX_PREDS = 5\n",
        "# Do not show labels with lower score than this:\n",
        "MIN_SCORE = 0.8 \n",
        "\n",
        "for features in ds_test.take(5):\n",
        "  image = features['image']\n",
        "  image = preprocess_image(image)\n",
        "  image = tf.image.resize(image, [CROP_TO, CROP_TO])\n",
        "\n",
        "  # Run model on image\n",
        "  logits = saved_module(image)\n",
        " \n",
        "  # Show image and predictions\n",
        "  show_preds(logits, image[0], correct_cifar_label=features['label'].numpy(), cifar_logits=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_y12tl_vj9m"
      },
      "source": [
        "## Summary\n",
        "\n",
        "In this post, you learned about the key components we used to train models that can transfer well to many different tasks. You also learned how to load one of our BiT models, fine-tune it on your target task and save the resulting model. Hope this helped and happy fine-tuning!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLUVIYWG10yX"
      },
      "source": [
        "## Acknowledgements\n",
        "\n",
        "This colab is based on work by Alex Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly and Neil Houlsby. We thank many members of Brain Research Zurich and the TensorFlow team for their feedback, especially Luiz Gustavo Martins and Marcin Michalski.\n"
      ]
    }
  ]
}

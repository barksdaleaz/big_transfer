# Big Transfer

This report and [this Jupyter Notebook](https://colab.research.google.com/drive/14VPdc1qjyFK__8DhyU8qSnYLwD45efzu?usp=sharing) were written and modified by Amber Barksdale and [Russell Wustenberg](https://github.com/OperaRuss) for the course CS_GY 6613: _Artificial Intelligence I_ in Spring 2021.

The report can also be found [in PDF form](https://github.com/barksdaleaz/big_transfer/blob/master/ASSIGNMENT/6613_AI_BigTransfer%20(1).pdf) in this repository.

## Summary of Transfer Learning

Children learn at an astonishing rate due in part to a phenomenon called transfer learning. In humans, transfer learning refers to the ability to generalize across a large number of classes and accurately identify objects---or more abstract concepts---based on as few as one single example [(Brown & Kane, 1988)](https://experts.umn.edu/en/publications/preschool-children-can-learn-to-transfer-learning-to-learn-and-le). If a child is shown a picture of a cow, they will then be able to extrapolate from that one image and point to cows on a farm. The child may also point to a giraffe because both cows and giraffes have spots and four legs; based on the two prominent features of a cow, this guess aligns with Edward Thorndike’s foundational theories that physical or perceptual similarity is necessary for transfer learning (Brown & Kane, 1988; [Thorndike 1913](https://psycnet.apa.org/record/2009-03129-000)). Within the realm of artificial intelligence and machine learning, “transfer learning” is based on the same concept of using prior knowledge in order to identify objects or complete tasks in a novel but similar domain. 

In stark contrast to humans, a traditional object-classification system requires a large number of training examples, and addresses isolated tasks [(Li, 2006)](https://www.semanticscholar.org/paper/Knowledge-transfer-in-learning-to-recognize-visual-Fei-Fei/35a198cc4d38bd2db60cda96ea4cb7b12369fd3c). Depending on the dimensionality of the image representations and the specific algorithm, the number of training examples could range from hundreds to thousands, resulting in an expensive and time-consuming computation process (Li, 2006). Transfer learning aims to mitigate this issue [(Torrey & Shavlik, 2009)](http://pages.cs.wisc.edu/~shavlik/abstracts/torrey.handbook09.abstract.html). To back up the claim that transfer learning does indeed improve learning, there are three metrics used: the initial performance using only transfer learning, the amount of time it takes to fully learn the task given transferred knowledge compared to the time it takes to learn the task from scratch, and the final performance level achievable in the target task compared to learning from scratch (Torrey & Shavlik, 2009). The different forms of knowledge transfer can be categorized by model parameters, feature or part sharing, or contextual information (Li, 2006). Contextual information refers to the fact that objects typically do not exist by themselves but rather surrounded by other objects that interact together. Use of the environment is common to help recognize objects (Li, 2006). Naturally, it has been observed that a learning agent in a real-life setting is more likely to encounter situations that require transfer learning (Torrey & Shavlik, 2009).

Unfortunately, it is possible for transfer learning to backfire. This undesirable outcome is called negative transfer, and causes performance to decrease (Torrey & Shavlik, 2009). Negative transfer may occur when the source task is not sufficiently related, or the relationship is not well leveraged by the transfer method (Torrey & Shavlik, 2009). Transfer methods should ideally produce positive transfer and avoid negative transfer in situations where the tasks are not a good match; however, this has been shown to be difficult to achieve simultaneously (Torrey & Shavlik, 2009). When safeguards are put in place to avoid negative transfer, there is a weaker positive effect, but aggressive transfer methods that produce large positive effects have no protections against negative transfer (Torrey & Shavlik, 2009). Three ways of avoiding negative transfer include rejecting bad information while learning the target task, choosing the best source task, and modeling the similarity between tasks and including this information in the transfer method (Torrey & Shavlik, 2009).

Transfer learning as a whole is a promising and desirable avenue in the realm of machine learning. Relevant challenges include avoiding negative transfer and automating task mapping, something easy to perform for humans but difficult to recreate in artificial intelligence (Torrey & Shavlik, 2009). The field of transfer learning is also moving toward enabling transfer between diverse tasks and performing transfer with more complex tasks. Overall, transfer learning is poised to become a standard in the field of machine learning and artificial intelligence.

## The Residual Network Structure of _Big Transfer_

Big Transfer (BiT) is the outcome of a Google Brains research project based out of Zürich. The goal was to produce a large generalist residual neural network (ResNet) that could be cheaply adapted to so-called _downstream_ specialized tasks. While developing their final models, the researchers investigated several aspects of neural network training and design, releasing their results in [(Kolesnikov et al., 2020)](https://arxiv.org/abs/1912.11370). Their choices in design optimization were driven by two main motivations: first, the massive size of the training data sets required careful consideration of hardware resources; second, the models have a high memory requirement, requiring small per-device batch sizes. Much can be learned about state-of-the-art neural network design by analysis of their design choices.

BiT consists of three trained models: BiT-S, for tasks with fewer than 20,000 training samples; BiT-M, for those with between 20,000 and 500,000; and BiT, for those with more than 500,000. It is worth noting that these labels are not correlated with the architecture size but rather the task and training data set. The creators of BiT trained all models on ‘vanilla’ ResNet-v2 architectures of various sizes with a number of modifications. They replaced all batch normalization (BN) layers with group normalization (GN) layers, all convolutional layers used Weight Standardization (WS), and used _mixup_ for medium and large tasks (Kolesnikov et al., 2020).

The choice to use a ResNet-v2 architecture (instead of the -v1) is motivated by [(He et al., 2016)](https://arxiv.org/abs/1603.05027). Following [(He et al., 2015)](https://arxiv.org/abs/1512.03385), which showed that residual functions were crucial to preventing weight decay to zero in a deep convolutional neural network, (He et al., 2016) asserts that identity mappings, and the specific sequencing of normalizations, weights, and activations, is crucial to leverage the benefits of a residual network.

In Kolesnikov et al., 2020, experiments were performed to analyze the relationship between ResNet scale and the dataset size. To that end, ResNet-50x1, -50x3, -101x1, -101x3 and -152x4 architectures were trained on various image classification tasks and their performance measured. A salient point was discovered that smaller models, given sufficiently large data sets, performed _worse_ than if they had been trained on a smaller data set. This has potential impact on past research that thought to benefit their model with a bounty of data without adjusting scale proportionally. Conversely, larger architectures performed better on all tasks, especially on severely constrained batch sizes (Kolesnikov et al., 2020).

The choice to exchange BN for GN was made in part due to hardware constraints (Kolesnikov et al., 2020). The per-unit memory constraints of the more expensive ResNet architectures caused the maximum batch size to be set low. BN has been shown to perform poorly under such conditions as well as requiring costly synchronization in parallel processing environments [(Ioffe, 2017)](https://arxiv.org/abs/1502.03167). GN, conversely, thrives in low batch conditions and is ideal for a parallel processing environment [(Wu & He, 2018)](https://arxiv.org/abs/1803.08494). As shown in [Zhang et al., 2018](https://arxiv.org/abs/1710.09412), the combination of GN with WS extends data normalization to the learning space and greatly reduces training time.

With the above normalizations in place, it may come as some surprise that the designers forwent the common regularization of dropout, weight decay to initial parameters or weight decay to zero (Kolesnikov et al., 2020). The designers claim that by using a sufficiently long training schedule a model will naturally converge to a loss threshold that is as though it were regularized. It should be noted, for those following along at home, that at least one of their larger models was allowed to run for 8 GPU _months_ to show sufficient results. This approach may not be tractable for all contexts [(Kilcher, 2020)](https://www.youtube.com/watch?v=k1GOF2jmX7c).

Finally, it is common to scale up the resolution of the image set between the training and test phases. Instead, Kolesnikov et al., 2020 advocates adding an alternative step: instead of being treated as an extension of the training set, the generalist model is fine-tuned to the test data set as though it were a specialist task. This emulates the transfer learning process as a whole, showing the model works as intended, and includes the increase in data resolution. 

## Three Applications of Normalization for NNs

Normalization has become a key technique for obtaining optimal learning rates, but there are many variants that to the neophyte may be confusing. Normalization has been shown to ease optimization, speed up training, decrease the importance of initial weight and smooth the loss landscape ([Santurkar et al., 2019;](https://arxiv.org/abs/1805.11604) Wu & He, 2018). Normalization is the redistribution of a set of data points in such a way that certain qualities are easier to distinguish when analyzing the set. Before normalization, a data set may consist of any set of values conceivable. After, all data points will exist between a set of known bounds, typically between 0 and 1. Most importantly, no information is lost in this transformation because the proportionality of each point to all other points is maintained. This is accomplished by treating each point as existing at a given _variance_ from the _mean_ of the data.

In a perfect world, the whole dataset would be fed into the network all at once. In reality, dataset sizes have now exceeded what one single machine could hold in memory at any time and the financial burden of operating such a machine would be prohibitively expensive. Batch Normalization (BN) is a natural solution to this problem. In BN, a number of data points are randomly pulled from the set of unprocessed data and normalized against each other. While the mean of the batch only approximates the mean of the whole dataset, successive batches will slowly converge upon the true value. This has the benefit of requiring fewer data samples to live in memory at one time and less ‘up front’ computation cost.

A neural network needs normalization not only before processing but internally at every node. As data passes through the network, each layer performs a function upon it, extracting information that will contribute to its final analysis. This has the unwanted effect of de-normalizing data at each internal layer. The researchers who developed BN showed that the most popular learning mechanism for neural networks, stochastic gradient descent, suffers when this _internal covariate shift_ occurs. The solution is to introduce normalization before each computational step. If each _unit_ within a version 2 residual network (ResNet-v2) can be generalized as 
EQUATION HERE. Equation taken from (He et al., 2015).

Then the output of each unit, $x_{out}$, will be computed by taking the input from the previous layer, _x-in_, normalizing the data, feeding it into an activation function, and applying the weight matrix to all values which were above the activation threshold. This right-side term will be repeated for every convolutional layer in this unit. Finally, the residual function, which is often simply the un-normalized value of _x-in_, is added to the result and passed as to the next layer.

During the learning phase of the training process, the degree of inaccuracy of the neural network’s final calculation is measured. This is interchangeably referred to as _loss_ or _error_. The network is informed of how correct and/or incorrect its final answer was, _propagates_ this score back through to each unit and adjusts its calculations accordingly. Since we are using a ResNet as the basis of our example, the amount of loss for each layer along an input pathway will travel along the _skip connections_ between residual units and deliver that unit’s contribution to the overall error of the system. The left-side term of our hypothetical unit allows a continuous stream of propagation backwards into the network while the right-side term is used to adjust the local calculations. Each unit computes its own error with respect to this overall score. Because the input into each layer is normalized, this process can be done faster and the calculations made more confidently by the system [(Ioffe & Szegedy, 2015)](https://arxiv.org/abs/1502.03167).

In a neural network where the data, the network architecture, and the computation hardware are all on one device, batch normalization works well. In many ways, it is the ‘gold standard’ for normalization techniques. However, it is not without its flaws. BN performs poorly when the batch size is small because error increases as the batch size becomes smaller, due to inaccurate batch statistics estimation (Wu & He, 2018). Many methods were proposed to resolve this issue, including (Ioffe, 2017; [Ren et al., 2017](https://arxiv.org/abs/1611.04520); [Ulyanov et al., 2017](https://arxiv.org/abs/1607.08022)). While some success was gained, all came at a cost to the overall performance of the network. That BN needs a sufficiently large batch results in high memory consumption, preventing researchers from exploring higher-capacity models that are limited by memory (Wu & He, 2018). This limits BN’s use in training larger models and for use in computer vision tasks such as detection and segmentation as these tasks have data sets with significant per-sample memory requirements (Wu & He, 2018). Finally, as network architectures have grown larger, there has been increased need for handling asynchrony across distributed systems. BN, with its requirement for batch synchronization at every layer, incurs inter-device synchronization cost (Kolesnikov et al., 2020).

Group Normalization, GN, is the synthesis of approaches to normalization that shows equal performance to BN while batch sizes are of moderate size and _maintains_ that level of performance even down to a batch size of one [Wu 2018]. GN grew out of a series of redefinitions of what is normalized within the batch, in particular Layer Normalization (LN) and Instance Normalization (IN). 
The figure below shows illustrations comparing BN, LN, IN and GN. Each cuboid shape is composed of three axes. $H,W$ represents an image input that has been flattened into a one-dimensional array of pixel values. _C_ represents the number of channels in a given convolutional layer. An image input to the system will begin with a default of 3 channels---red, green, and blue---and more will develop as the channel input is convolved in future network layers. _N_ represents the number of samples. As can be seen, BN calculates the mean and variance of all pixel values for a _single_ channel across the entire batch of size _N_.

![group norm](https://github.com/barksdaleaz/big_transfer/blob/master/20210311_GN_Boxes.png)

Cuboid illustrations of feature map tensors where $N$ is the batch axis, _C_ is the channel axis, and _(H,W)_ are the spatial axes. Pixels in blue are normalized by the same mean and variance. Figure from Wu & He, 2018.

LN proposed to solve this issue by limiting its scope to only a single data sample but normalizing all points across all channels of that sample. The flaw in this method is that LN will take a complex sample, such as an image, and find one _universal_ mean and variance for the entire image. This is computationally expensive and does not permit sensitivity to variance between channels (Wu & He, 2018). The creators of IN, seeking to maintain the independence of the channels, normalized each channel of a data sample independently. For example, IN would only calculate the mean and variance of only the red, green or blue channel of an image. Ultimately, neither LN nor IN had been able to approach BN’s accuracy in many visual tasks, but the creators saw them as a point of inspiration for the design of GN.

GN maintains the innovation of only utilizing a single sample as the basis for normalization. This choice makes it agnostic to batch size and, theoretically, it will perform as well on a single-sample batch as on a large batch. Rather than an ‘all-or-nothing’ approach on channels, the algorithm divides the channels into a number of groups determined by a hyperparameter, _Gamma_. In Wu & He, 2018, the default value is 32. This hyperparameter allows for experimentation during training. When Gamma = 1 all channels are treated as in LN; when _Gamma = C_, where _C_ is the number of channels, then GN becomes IN Wu & He, 2018. In combination with weight standardization, it has been shown to be useful in transfer learning applications and to even outperform BN under certain conditions (Kolesnikov et al., 2020).

Normalization can also be applied to the weight matrices themselves, and doing so is extremely beneficial for learning. Deep neural networks without some form of weight normalization struggle to learn for a number of theorized reasons (Huang, 2017). Several attempts were made to combat this, including research into weight initialization values (cite{Glorot_DifficultyOfTrainingDeep}), learned weight normalization vectors (cite{Salimans_WeightNormalization}) and the ominously named _pathological curvature_ (cite{Huang_CenteredWeightNormalization}).

By far the most successful is referred to as Weight Standardization (WS) which was developed to avoid _singularities_ in the neural network and guarantee a _smoothness_ of values. Smoothness, in the context of a deep neural network, refers to the ease with which the network will be able to discover the way to adjust the weights in respect to its loss. When a network is unsmooth, the learning rate must be set low or else the network may make a wrong choice in adjusting its weights. Singularities occur when a weight approaches, equals, or is less than zero. Singularities cause a network’s learning rate to drop rapidly (cite{Wei_LinearLearningNearSingularities}).

The weights, as with the data, will begin as a set of arbitrary values and change over the course of training. If left unregulated, the network will struggle to learn. To solve this issue, WS separates the actual weight values with a matrix of normalized weights, which are calculated on the original values. This avoids the risk of singularity as it is not the specific weight values but rather their distribution that matters to the calculations. Each pass of the learning algorithm will adjust the distribution of weights rather than their raw values. 

In essence, BN, GN and WS are all extensions of the same principle for deep neural learning. As has been shown, neural networks universally benefit when inputs are drawn from a smooth sample space. When working with data, batch normalization has proven to be a successful technique. Group normalization, seeking to handle contexts in which BN behaves poorly, is especially beneficial when batch sizes are small and data may only be partially in memory. Weight standardization, inspired by the application of normalization in data space, applies normalization to the weight matrix. Variations exist on these techniques, and research is ongoing to optimize deep neural networks as increasing layers and complex learning topologies emerge.

## Summary of _mixup_ for NN Regularization

As was seen in the previous section, deep neural networks benefit greatly from smooth topologies in data and learning space. Most often, machine learning designers achieve smoothness by normalization. In the evolution of batch, layer, instance and group normalization techniques, several attempts must be made to find an optimal normalization approach given practical constraints. _mixup_ is a method of data normalization shown to have an impact on a number of areas requiring regularization (\cite{Carratino_MixupRegularization}). _mixup_ recontextualizes the data provided to the network by randomly interpolating virtual data points between known data points (\cite{Zhang_MixUp}). Following _mixup_, a network is able to evaluate its hypotheses with respect to their vicinity to known ground truths rather than empirical error measurements.

Traditionally in supervised learning, a neural network is provided a set of training data along with the correct labels correlating with each sample. The data is fed into the network for classification, treated as a hypothesis, and the resulting predictions compared to the labels as a ground truth. Empirical Risk Measurement (ERM) measures the distance between each hypothesis made and the ground truth and calls this distance error (\cite{Vapnik_StatisticalLearningTheory}). With its dependence on provided data, a network trained on unmodified ERM tends to draw strong class definitions within the sample space. This can lead to two major weaknesses in a finalized model: \textit{memorization} and openness to adversarial attack (\cite{Zhang_MixUp}).
	
_mixup_ proposes to solve such problems by constructing virtual training examples from those provided at training time. Using an inexpensive computation, _mixup_ takes known data points with confident ground truths and interpolates virtual points with ground truths relative to their vicinity to known points. When a network hypothesizes such an ambiguous result, the network can reference the virtual point and affirm that such an interpretation has a valid basis given the data. The result is a sort of ‘Gaussian noise’ where data is seen as a smooth probabilistic landscape rather than a set of discrete definitions (\cite{Zhang_MixUp}).

These virtual training points are constructed using \textit{Vicinal Risk Minimization} (VRM) (\cite{Chapelle_VicinalRiskMinimization}). VRM translates each known data point into a probability distribution with a \textit{vicinity of likelihood}. By sampling this distribution, the network is now attempting to learn by minimizing the \textit{empirical vicinal risk} rather than simply \textit{empirical risk}. VRM introduces proximity to the data, and the network can utilize this spatiality to orient itself within the data space.

Given two known data points, $x_i$ and $x_j$, with known labels, $y_i$ and $y_j$, the virtual point, $\tilde{x}$, and label $\tilde{y}$ is interpolated at a proportional distance, $\lambda$, between the two points.  In equation form, _mixup_ is written as
\begin{align*}
\tilde{x} &= \lambda x_i + (1-\lambda )x_j\\
\tilde{y} &= \lambda y_i + (1-\lambda ) y_j
\end{align*}
The resulting virtual point is a linear combination of the known data points and labels. When sampled by the network, its label will exist in the liminal space between the two discrete examples. When sampling the error in respect to this virtual point, EVM will now report smaller (i.e.. smoother) adjustments back to the network. This has been shown to speed network learning and produce more confident likelihood reporting in generalization (\cite{Carratino_MixupRegularization}).
	
_mixup_ provides a number of explicit and implicit regularizations to the network. The original motivation for its creation was to alleviate overfitting during neural network training (\cite{Zhang_MixUp}). By extension, adversarial attack is mitigated as formerly deceptive samples with ambiguous content are now viewed within context and the data space is less exploitable due to harsh definition boundaries. A number of other regularizing effects have been noted, including model calibration, increased robustness to input adversarial noise, and robustness to label corruption (\cite{Thulasidasan_MixUp_ImprovedCalibration, Zhang_MixUp}. Traditionally, amelioration of these issues has required separate methods, but _mixup_, in a deceptively intuitive procedure, cohesively regularizes the data. Variations on _mixup_ have been researched, all showing positive impact on performance (\cite{Guo_MixupAsLocallyLinear, Verma_ManifoldMixup, Yun_CutMix}. Some researchers have cautioned against using the process when training set sizes are below a certain threshold (\cite{BiT_Article}).

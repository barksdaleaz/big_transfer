## Summary of Transfer Learning

Children learn at an astonishing rate due in part to a phenomenon called transfer learning. In humans, transfer learning refers to the ability to generalize across a large number of classes and accurately identify objects---or more abstract concepts---based on as few as one single example [(Brown & Kane, 1988)](https://experts.umn.edu/en/publications/preschool-children-can-learn-to-transfer-learning-to-learn-and-le). If a child is shown a picture of a cow, they will then be able to extrapolate from that one image and point to cows on a farm. The child may also point to a giraffe because both cows and giraffes have spots and four legs; based on the two prominent features of a cow, this guess aligns with Edward Thorndike’s foundational theories that physical or perceptual similarity is necessary for transfer learning (Brown & Kane, 1988; [Thorndike 1913](https://psycnet.apa.org/record/2009-03129-000)). Within the realm of artificial intelligence and machine learning, “transfer learning” is based on the same concept of using prior knowledge in order to identify objects or complete tasks in a novel but similar domain. 

In stark contrast to humans, a traditional object-classification system requires a large number of training examples, and addresses isolated tasks [(Li, 2006)](https://www.semanticscholar.org/paper/Knowledge-transfer-in-learning-to-recognize-visual-Fei-Fei/35a198cc4d38bd2db60cda96ea4cb7b12369fd3c). Depending on the dimensionality of the image representations and the specific algorithm, the number of training examples could range from hundreds to thousands, resulting in an expensive and time-consuming computation process (Li, 2006). Transfer learning aims to mitigate this issue [(Torrey & Shavlik, 2009)](http://pages.cs.wisc.edu/~shavlik/abstracts/torrey.handbook09.abstract.html). To back up the claim that transfer learning does indeed improve learning, there are three metrics used: the initial performance using only transfer learning, the amount of time it takes to fully learn the task given transferred knowledge compared to the time it takes to learn the task from scratch, and the final performance level achievable in the target task compared to learning from scratch (Torrey & Shavlik, 2009). The different forms of knowledge transfer can be categorized by model parameters, feature or part sharing, or contextual information (Li, 2006). Contextual information refers to the fact that objects typically do not exist by themselves but rather surrounded by other objects that interact together. Use of the environment is common to help recognize objects (Li, 2006). Naturally, it has been observed that a learning agent in a real-life setting is more likely to encounter situations that require transfer learning (Torrey & Shavlik, 2009).

Unfortunately, it is possible for transfer learning to backfire. This undesirable outcome is called negative transfer, and causes performance to decrease (Torrey & Shavlik, 2009). Negative transfer may occur when the source task is not sufficiently related, or the relationship is not well leveraged by the transfer method (Torrey & Shavlik, 2009). Transfer methods should ideally produce positive transfer and avoid negative transfer in situations where the tasks are not a good match; however, this has been shown to be difficult to achieve simultaneously (Torrey & Shavlik, 2009). When safeguards are put in place to avoid negative transfer, there is a weaker positive effect, but aggressive transfer methods that produce large positive effects have no protections against negative transfer (Torrey & Shavlik, 2009). Three ways of avoiding negative transfer include rejecting bad information while learning the target task, choosing the best source task, and modeling the similarity between tasks and including this information in the transfer method (Torrey & Shavlik, 2009).

Transfer learning as a whole is a promising and desirable avenue in the realm of machine learning. Relevant challenges include avoiding negative transfer and automating task mapping, something easy to perform for humans but difficult to recreate in artificial intelligence (Torrey & Shavlik, 2009). The field of transfer learning is also moving toward enabling transfer between diverse tasks and performing transfer with more complex tasks. Overall, transfer learning is poised to become a standard in the field of machine learning and artificial intelligence.

## The Residual Network Structure of _Big Transfer_

Big Transfer (BiT) is the outcome of a Google Brains research project based out of Zürich. The goal was to produce a large generalist residual neural network (ResNet) that could be cheaply adapted to so-called _downstream_ specialized tasks. While developing their final models, the researchers investigated several aspects of neural network training and design, releasing their results in [(Kolesnikov et al., 2020)](https://arxiv.org/abs/1912.11370). Their choices in design optimization were driven by two main motivations: first, the massive size of the training data sets required careful consideration of hardware resources; second, the models have a high memory requirement, requiring small per-device batch sizes. Much can be learned about state-of-the-art neural network design by analysis of their design choices.

BiT consists of three trained models: BiT-S, for tasks with fewer than 20,000 training samples; BiT-M, for those with between 20,000 and 500,000; and BiT, for those with more than 500,000. It is worth noting that these labels are not correlated with the architecture size but rather the task and training data set. The creators of BiT trained all models on ‘vanilla’ ResNet-v2 architectures of various sizes with a number of modifications. They replaced all batch normalization (BN) layers with group normalization (GN) layers, all convolutional layers used Weight Standardization (WS), and used _mixup_ for medium and large tasks (Kolesnikov et al., 2020).

The choice to use a ResNet-v2 architecture (instead of the -v1) is motivated by [(He et al., 2016)](https://arxiv.org/abs/1603.05027). Following [(He et al., 2015)](https://arxiv.org/abs/1512.03385), which showed that residual functions were crucial to preventing weight decay to zero in a deep convolutional neural network, (He et al., 2016) asserts that identity mappings, and the specific sequencing of normalizations, weights, and activations, is crucial to leverage the benefits of a residual network.

In Kolesnikov et al., 2020, experiments were performed to analyze the relationship between ResNet scale and the dataset size. To that end, ResNet-50x1, -50x3, -101x1, -101x3 and -152x4 architectures were trained on various image classification tasks and their performance measured. A salient point was discovered that smaller models, given sufficiently large data sets, performed _worse_ than if they had been trained on a smaller data set. This has potential impact on past research that thought to benefit their model with a bounty of data without adjusting scale proportionally. Conversely, larger architectures performed better on all tasks, especially on severely constrained batch sizes (Kolesnikov et al., 2020).

The choice to exchange BN for GN was made in part due to hardware constraints (Kolesnikov et al., 2020). The per-unit memory constraints of the more expensive ResNet architectures caused the maximum batch size to be set low. BN has been shown to perform poorly under such conditions as well as requiring costly synchronization in parallel processing environments [(Ioffe, 2017)](https://arxiv.org/abs/1502.03167). GN, conversely, thrives in low batch conditions and is ideal for a parallel processing environment [(Wu & He, 2018)](https://arxiv.org/abs/1803.08494). As shown in [Zhang et al., 2018](https://arxiv.org/abs/1710.09412), the combination of GN with WS extends data normalization to the learning space and greatly reduces training time.

With the above normalizations in place, it may come as some surprise that the designers forwent the common regularization of dropout, weight decay to initial parameters or weight decay to zero (Kolesnikov et al., 2020). The designers claim that by using a sufficiently long training schedule a model will naturally converge to a loss threshold that is as though it were regularized. It should be noted, for those following along at home, that at least one of their larger models was allowed to run for 8 GPU _months_ to show sufficient results. This approach may not be tractable for all contexts [(Kilcher, 2020)](https://www.youtube.com/watch?v=k1GOF2jmX7c).

Finally, it is common to scale up the resolution of the image set between the training and test phases. Instead, Kolesnikov et al., 2020 advocates adding an alternative step: instead of being treated as an extension of the training set, the generalist model is fine-tuned to the test data set as though it were a specialist task. This emulates the transfer learning process as a whole, showing the model works as intended, and includes the increase in data resolution. 


